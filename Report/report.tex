%%%%%%%% DATA LITERACY 2025 LATEX PROJECT TEMPLATE FILE %%%%%%%%%%%%%%%%%
%%% Based on the 2025 ICML template, available at https://icml.cc/Conferences/2025/AuthorInstructions %%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

\definecolor{c11}{RGB}{73,232,91}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{enumitem}
% \setlist{noitemsep}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Report Template for Data Literacy 2025}

\begin{document}

\twocolumn[
\icmltitle{My Data Literacy Project\\ (Replace this with your Project Title)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% Alphabetic order, equal contribution anyway.
\icmlauthor{Zhi Jing}{equal,first}
\icmlauthor{Enrico Quinto}{equal,second}
\icmlauthor{T\`ai Th\'ai}{equal,third}
\icmlauthor{Jonas Thumbs}{equal,fourth}
\icmlauthor{Baisu Zhou}{equal,fifth}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
\icmlaffiliation{first}{Matrikelnummer 12345678, MSc Machine Learning}
\icmlaffiliation{second}{Matrikelnummer 12345678, Exchange(?)}
\icmlaffiliation{third}{Matrikelnummer 12345678, MSc Machine Learning}
\icmlaffiliation{fourth}{Matrikelnummer 12345678, MSc Machine Learning}
\icmlaffiliation{fifth}{Matrikelnummer 7264384, MSc Machine Learning}

% put your email addresses here. You can use initials to save space, 
% e.g. if you are called Max Mustermann, you can use \icmlcorrespondingauthor{MM}{max.mustermann@uni-tuebingen.de}
% DO USE YOUR UNIVERSITY EMAIL ADDRESS!
\icmlcorrespondingauthor{ZJ}{first1.last1@uni-tuebingen.de} 
\icmlcorrespondingauthor{EQ}{first2.last2@uni-tuebingen.de}
\icmlcorrespondingauthor{TT}{first3.last3@uni-tuebingen.de}
\icmlcorrespondingauthor{JT}{first4.last4@uni-tuebingen.de}
\icmlcorrespondingauthor{BZ}{baisu.zhou@student.uni-tuebingen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
TODO.
\end{abstract}

\section{Introduction}\label{sec:intro}

Measuring color accurately is a common problem in daily life, e.g. when a surface needs to be repainted in the same color. Devices that can carry out this task, the so-called \emph{colorimeters}, are available commercially and usually cost several hundred dollars. 

On the other hand, there are smartphone apps that claim to offer the same service for free by using the built-in camera. A major challenge in this approach is how to deal with the effect of ambient light. Simply picking the color from a photo is not ideal because hue, saturation and brightness depend heavily on the lighting conditions. One way to address this issue is to capture the color of a white reference object (e.g., a white piece of paper) at the same time and use this to correct for the ambient light.

In this study, we want to find out the best approach to correct for ambient light and determine how accurate the measurements from such an app can be. In order to do this, we collected a dataset, explored several algorithms for color correction and calculated the accuracy of the measurements with respect to a suitable metric.

\section{Data and Methods}\label{sec:methods}

In this section, we discribe the data collection procedure and define two color correction methods which we compare in subsequent analysis.

\subsection{Data Collection}\label{sec:data}

We used a color reference sheet designed for camera calibration as the object from which we collect data.
The reference sheet consists of 24 color cards and provides accurate ground-truth RGB values for each color.
We used 4 different phones, Google Pixel 8 Pro, Samsung Galaxy A55, Samsung Galaxy S21 Ultra, Motorola g72 for data collection.
The data were collected indoors on a desk in front of the window under natural daylight on November 19, 2025 in Tübingen, Germany.
The procedure is as follows.

We placed a white piece of paper (called \emph{white reference} in the following) with a cut-out square hole over the array of colors to ensure similar conditions for all measurements.
Only one color card was visible to the camera at any time.
We took 10 photos for each color and stored the pixel RGB values at the locations of both reticles.\todo{
    Perhaps a screenshot here can help.
}
Additionally, we also stored the mean colors of a $5 \times 5$ and $9 \times 9$ pixel squares at the same locations.\todo{
    Do we have space left to discuss the reticle size?
}
After that, we moved to the next color until we collected data for all 24 colors.
We wanted to make the procedure of taking the photos as similar as possible for the 24 colors, so all measurements in one run were taken by one person.
We also tried to take images with a similar distribution of angles and distances over the 24 colors.
The persons who took the images were told not to vary the yaw angle during dataset collection, since the app only detects pitch and roll.
% Taking one dataset of $10 \times 24$ images took between 10 and 20 minutes, so the daylight might vary slightly, but not strongly over that time.
The dataset we use in subsequent analysis consist of 480 measurements taken in two runs.

% lighting conditions: daylight through window, dark room (no lights, window shades closed) and extreme red, green, and blue light provided by a muliticolor LED light bulb. 

\subsection{Correction by Scaling}

We first consider a simple, deterministic correction algorithm.
Let $(R_\textup{m}, G_{\textup{m}}, B_\textup{m})$ be the measured RGB values of a color card.
For the scaling method, we assume that the measured color $(R_\textup{w}, G_{\textup{w}}, B_\textup{w})$ of the white reference is the brightness color one can measure under the ambient light.
To recover the full RGB space from the measurable color space $[0, R_\textup{w}] \times [0, G_\textup{w}] \times [0, B_\textup{w}]$, we apply a channelwise rescaling.
The corrected RGB values are given by
\begin{equation*}
    \begin{pmatrix}
        \widehat{R} \\ 
        \widehat{G} \\
        \widehat{B}
    \end{pmatrix} = \begin{pmatrix}
        \frac{255}{R_\textup{w}} & 0 & 0 \\
        0 & \frac{255}{G_\textup{w}} & 0 \\
        0 & 0 & \frac{255}{B_\textup{w}}
    \end{pmatrix}\begin{pmatrix}
        R_\textup{m} \\
        G_\textup{m} \\
        B_\textup{m}
    \end{pmatrix}.
\end{equation*}
This method was suggested by TODO:REF for colorimetry.

\subsection{Correction by Model}

The second method is to fit a regression model on our measured data.
For this method, we first need to transform the colors into a meaningful color space.
The RGB space is not suitable, because the Euclidean distance between RGB triples does not align with the perceived difference between colors.
In our analysis, we use the CIELAB (or $L^* a^* b^*$) color space, designed by the Commission Internationale de l’Éclairage (CIE)\footnote{
    In Enligh: International Commission on Illumination.
    Website: \url{https://cie.co.at/}.
} for colorimetric applications.
The $L^*$ coordinate represents lightness, while $a^*$ and $b^*$ control the hue.
CIELAB was designed to provide approximately perceptual uniformity, with scales comparable to those of color atlases such as the Munsell system, and with coordinate axes corresponding to Hering’s opponent-color theory \cite{oleari2015}.
This allows the use of Euclidean distance and related metrics, such as the mean squared error (MSE), to fit and analyze the model.

We consider a multi-target linear model defined by
\begin{equation} \label{eq:model}
\begin{split}
    \begin{pmatrix}
        \widehat{L^*} \\ 
        \widehat{a^*} \\
        \widehat{b^*}
    \end{pmatrix}
    =& \begin{pmatrix}
        A_{11} & A_{12} & A_{13} \\
        A_{21} & A_{22} & A_{23} \\
        A_{31} & A_{32} & A_{33}
    \end{pmatrix} \begin{pmatrix}
        L^*_\textup{m} \\
        a^*_\textup{m} \\
        b^*_\textup{m}
    \end{pmatrix} + \begin{pmatrix}
        b_1 \\
        b_2 \\
        b_3
    \end{pmatrix} \\
    &+ \begin{pmatrix}
        C_{11} & C_{12} & C_{13} \\
        C_{21} & C_{22} & C_{23} \\
        C_{31} & C_{32} & C_{33}
    \end{pmatrix} \begin{pmatrix}
        L^*_\textup{w} \\
        a^*_\textup{w} \\
        b^*_\textup{w}
    \end{pmatrix} \\
    &+ \begin{pmatrix}
        D_{11} & D_{12} \\
        D_{21} & D_{22} \\
        D_{31} & D_{32}
    \end{pmatrix} \begin{pmatrix}
        \textup{pitch} \\
        \textup{roll}
    \end{pmatrix},
\end{split}
\end{equation}
where $(L^*_\textup{m}, a^*_\textup{m}, b^*_\textup{m})$ and $(L^*_\textup{w}, a^*_\textup{w}, b^*_\textup{w})$ are the measured color of a color card and the white reference, respectively, and $A_{ij}, b_i, C_{ij}, D_{ij}$ are parameters.
The $L^* a^* b^*$ values for color cards and white reference are obtained by a non-linear transformation from the RGB values.
The transformation is intended for measurements under the standard indoor daylight condition D65, defined by CIE \cite{oleari2015}.
The lighting condition under which we collected data was an approximation to the D65 condition.
In \cref{tab:white-comparison} we compare the $L^* a^* b^*$ values of the white color under the D65 condition and the white reference used in our experiment.
The measured $L^*$ value is much lower than the D65 value, because the white reference in use is not perfectly white.
We assume the deviations in $a^*$ and $b^*$ from the standard case are sufficiently small and the transformation from RGB to $L^* a^* b^*$ is still valid.

\begin{table}
\caption{Comparison between the standard white in D65 and the white reference in our measurements (mean $\pm$ standard deviation).}
\label{tab:white-comparison}
\centering
\begin{tabular}{rccc}
    \toprule
    & $L^*$ & $a^*$ & $b^*$ \\
    \midrule
    \textbf{D65} & $100.000$ & $0.005260$ & $-0.010408$ \\
    \textbf{our} & $77.21 \pm 3.85$ & $0.30 \pm 0.93$ & $-3.53 \pm 2.07$ \\
    % \textbf{std} & $3.85248$ & $0.934964$ & $2.06641$ \\
    \bottomrule
\end{tabular}
\end{table}

Pitch and roll in \eqref{eq:model} provide information about the angle from which a picture was taken.
Our assumption is that the white reference can sufficiently characterize the ambient light, so that the additional information of pitch and roll contributes little to color correction.
We validate this assumption by comparing the full model \eqref{eq:model} with a reduced model with $D = 0$.

We fit the full and reduced model by minimizing the empirical risk associated with the loss function
\begin{equation*} %\label{eq:euclidean-error}
    \ell(y, \hat{y})
    = \left( L^* - \widehat{L^*} \right)^2 + \left( a^* - \widehat{a^*} \right)^2 + \left( b^* - \widehat{b^*} \right)^2,
\end{equation*}
where $y = (L^*, a^*, b^*)$ denotes the ground truth color and $\hat{y} = (\widehat{L^*}, \widehat{a^*}, \widehat{b^*})$ denotes the correction given by the model.
For simplicity, we refer to $\sqrt{\ell(y, \hat{y})}$, the Euclidean distance between $y$ and $\hat{y}$, as the \emph{Euclidean error} of a correction, and the average of Euclidean error across training or test data as the \emph{mean squared error (MSE)}.\todo{
    Think about what to do with factor 3.
}
For the purpose of uncertainty quantification,
we construct bootstrap confidence intervals \cite{davison1997,efron1994} for the estimated coefficients.
We resample observations with replacement\footnote{%
    This corresponds to ``resampling cases'' in \citet{davison1997} and ``bootstrap pairs'' in \citet{efron1994}.
} and construct a percentile bootstrap confidence interval per parameter.

While our dataset covers only 24 colors, the correction methods ought to generalize to arbitrary colors.
We simulate the scenario of generalization to unseen colors by employing a $k$-fold cross-validation (CV) strategy.
For each $k \in \{1,\dots,20\}$, we reserve $k$ colors for testing and use the remaining $24 - k$ colors for training.\footnote{%
    Since our goal is not to identify model configurations of best generalization performance,
    but to analyze the model's behavior regarding unseen colors,
    we use all data in the CV process.
}
This CV scheme yields $\sum_{k=1}^{20} \binom{24}{k}$ trials in total.
As it is infeasible to exhaust all possible configurations, we consider two approaches to selecting colors to leave out:
\begin{enumerate}[nosep]
    \item We randomly select $k$ colors.
    \item We manually select groups of colors based on their perceived tone (see \cref{tab:cv-folds}).
\end{enumerate}
Furthermore, we perform leave-one-out CV, training the model on all but the $k$-th color for $k = 1,\dots,24$ and testing it on the single color left out.


\begin{table}
\caption{Targeted leave-out cross validation specification.}
\label{tab:cv-folds}
\centering
\begin{tabular}{ll}
    \toprule
    Test set & Colors \\
    \midrule
    Red & 7, 9, 12, 15 \\
    Green & 4, 6, 11, 14 \\
    Blue & 3, 5, 6, 8, 13, 18 \\
    Neutral & 19-24 \\
    \bottomrule
\end{tabular}
\end{table}


\section{Results}\label{sec:results}

\subsection{Scaling vs.\@ Model}

\begin{figure}
    \includegraphics{../Images/plot_error_ecdf.pdf}
    \caption{Euclidean error ECDFs of the correction methods with $95\%$ Dvoretzky–Kiefer–Wolfowitz confidence band \cite{dvoretzky1956,wasserman2004}.}
    \label{fig:error-ecdf}
\end{figure}

We apply scaling correction to the same test set on which we evaluate the full and reduced models.
\cref{fig:error-ecdf} shows the empirical cumulative distribution function (ECDF) of the measurement-wise Euclidean error in the $L^* a^* b^*$ space.
In comparison to the raw measurements, our proposed correction methods result in an overall reduction of Euclidean error.
The error ECDFs of the two model variants are almos identical.
This similarity of error distributions indicates that adding information about pitch and roll does not change the behavior of the model, as we expected.
While the error ECDF of the models lie above that of the scaling method,
their confidence bands largely overlap.
However, the maximum error incurred by either model is lower than the scaling method.

In order to gain some insights into how the correction methods reduce the Euclidean error, we visualize the error per channel in \cref{fig:error-kde}.
As the two model variants yield comparable results, we only display the reduced model.
One can observe a shift of error distribution in the $L^*$-channel towards zero resulted from the correction methods,
whereas the error distributions in the other two channels are all centered at approximately zero.
These observations show that the correction methods are capable of removing bias towards darker colors (which is possibly due to the ambient light), but cannot eliminate uncertainty in hue.


\begin{figure*}
    \includegraphics{../Images/plot_error_kde.pdf}
    \caption{Kernel density plots for channel-wise difference from ground truth.}
    \label{fig:error-kde}
\end{figure*}

% \begin{table}
% \caption{%
%     Statistics of Euclidean error in raw measurements and after correction.
%     Relative change to raw data (first row) are given in parentheses.
% }
% \label{tab:error-ecdf-stats}
% \centering
% \begin{tabular}{crrrr}
%     \toprule
%     Method & Mean & Median & Max \\
%     \midrule
%     (Raw) & 20.71 (0\%) & 21.46 (0\%) & 45.67 (0\%) \\
%     Scaling & 11.63 (-44\%) & 9.71 (-55\%) & 20.87 (-35\%) \\
%     Full model & 8.08 (-61\%) & 7.19 (-66\%) & 17.70 (-61\%) \\
%     Reduced model & 8.16 (-61\%) & 7.17 (-67\%) & 17.31 (-62\%) \\
%     \bottomrule
% \end{tabular}
% \end{table}

% \begin{itemize}
%     \item (Grid comparison: visual sanity check, no clear pattern cross colors. $\to$ visual abstract?)
%     \item (HSV or rather LAB) KDE plots: slightly more quantitative, but still interpretable.
%     \item Raw vs.\@ corrected RGB/LAB: scaling mostly corrects for brightness. The gray scale colors reflect the ambient light -- balanced light, as expected by the standard lighting condition.
% \end{itemize}
% KDE plots:
% \begin{itemize}
%     \item Both scaling and model mostly correct for brightness!
%     \item Hue correction is minimal, possibly because of the ambient light.
% \end{itemize}

\subsection{Cross Validation}

\todo[inline]{Take RMSE instead of MSE?}

The results indicate the MSE remains stable across a wide range of training sets sizes.
A significant inflection point in performance is observed at leaving out $k = 19$ colors from the training set,
where the MSE increases drastically.
This behavior suggests that the model maintains high predictive accuracy and robustness even when trained on a small number of colors.
We therefore believe that our results generalize beyond the 24 colors we collected data from.
% Consequently, the experimental results are highly reproducible, requiring a minimal threshold of only 5 to 6 training samples to achieve stability.

In the targeted leave-out experiments (see \cref{tab:cv-folds}), we observe that the model trained on all colors but the green ones fail to generalize to the green colors,
while the model readily generalizes to red or blue colors even if they are left out.
The results of leave-one-out CV confirm that color 11 (\textcolor{c11}{yellow green}) is the primary driver of failure of generalization.
Notably, color 11 is both a member of the Greenish group and possesses the largest green ($G$) channel intensity in the dataset.

The significant surge in MSE upon the omission of greenish samples—and Sample 11 in particular—can be attributed to the non-linearities inherent in the sensor’s Green-channel response or the spectral distribution of the target illuminant. In many Bayer-filter-based sensors, the Green channel captures the majority of the luminance information and serves as the primary anchor for the transformation matrix.
If the training set lacks these high-intensity Green samples, the model fails to capture the curvature of the sensor's response in that region, leading to poor extrapolation. Essentially, the Green samples represent a critical boundary condition; without them, the transformation becomes ill-conditioned for a large portion of the visible gamut.
\todo[inline]{Citation!}

\section{Discussion \& Conclusion}\label{sec:conclusion}

% \begin{itemize}
%     \item Limitations:
%     \begin{itemize}
%         \item Analysis focused on one lighting condition. We could not exactly reproduce D65 due to limited resources.
%         In particular, professional colorimeter as comparison would be meaningful.
%         \item White paper is not (255, 255, 255) -- bias towards underestimating brightness.
%         \item Factors not considered: type of smartphone, distance to object.
%         \item Error in color remains mostly uninterpretable.
%         Only comparison, but no statement about whether one method is ``good enough.''
%     \end{itemize}
% \end{itemize}

\newpage

\section*{Contribution Statement}

TODO: Explain here, in one sentence per person, what each group member contributed.

% \section*{Notes} 

% Your entire report has a \textbf{hard page limit of 4 pages} excluding references and the contribution statement. (I.e. any pages beyond page 4 must only contain the contribution statement and references). Appendices are \emph{not} possible. But you can put additional material, like interactive visualizations or videos, on a githunb repo (use \href{https://github.com/pnkraemer/tueplots}{links} in your pdf to refer to them). Each report has to contain \textbf{at least three plots or visualizations}, and \textbf{cite at least two references}. More details about how to prepare the report, inclucing how to produce plots, cite correctly, and how to ideally structure your github repo, will be discussed in the lecture, where a rubric for the evaluation will also be provided.


\bibliography{bibliography}
\bibliographystyle{icml2025}

\end{document}

% This document was modified from the files available at https://icml.cc/Conferences/2025/AuthorInstructions
% the full copyright notice is available within the file icml2025.sty