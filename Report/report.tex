%%%%%%%% DATA LITERACY 2025 LATEX PROJECT TEMPLATE FILE %%%%%%%%%%%%%%%%%
%%% Based on the 2025 ICML template, available at https://icml.cc/Conferences/2025/AuthorInstructions %%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

\definecolor{c11}{RGB}{73,232,91}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Report Template for Data Literacy 2025}

\begin{document}

\twocolumn[
\icmltitle{My Data Literacy Project\\ (Replace this with your Project Title)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% Alphabetic order, equal contribution anyway.
\icmlauthor{Zhi Jing}{equal}
\icmlauthor{Enrico Quinto}{equal}
\icmlauthor{T\`ai Th\'ai}{equal}
\icmlauthor{Jonas Thumbs}{equal}
\icmlauthor{Baisu Zhou}{equal}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
% \icmlaffiliation{first}{Matrikelnummer 12345678, MSc Machine Learning}
% \icmlaffiliation{second}{Matrikelnummer 12345678, MSc Computer Science}
% \icmlaffiliation{third}{Matrikelnummer 12345678, MSc Media Informatics}
% \icmlaffiliation{fourth}{Matrikelnummer 12345678, MSc Medical Informatics}
% \icmlaffiliation{fifth}{Matrikelnummer 12345678, MSc QDS}

% put your email addresses here. You can use initials to save space, 
% e.g. if you are called Max Mustermann, you can use \icmlcorrespondingauthor{MM}{max.mustermann@uni-tuebingen.de}
% DO USE YOUR UNIVERSITY EMAIL ADDRESS!

% for the Data Literacy report, to save space, you can here list the student email address of one author, who is willing to be contacted about this work in the future (e.g. in case we would like to use your report as an example for future course iterations)
\icmlcorrespondingauthor{BZ}{baisu.zhou@student.uni-tuebingen.de} 
% \icmlcorrespondingauthor{Initials2}{first2.last2@uni-tuebingen.de}
% \icmlcorrespondingauthor{Initials3}{first3.last3@uni-tuebingen.de}
% \icmlcorrespondingauthor{Initials4}{first4.last4@uni-tuebingen.de}
% \icmlcorrespondingauthor{Initials5}{first5.last5@uni-tuebingen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
While colorimetry devices can accurately measure colors of objects, they are too expensive for daily usage such as detecting color of scratched surfaces for repainting.
Color picking from images captured by a smartphone is subject to distortion due to ambient light.
We statistically analyze a deterministic and a data-driven color correction method which improve the accuracy of color detection using smartphone images,
showing that simple, post-hoc correction can greatly improve the quality of color measurements done by smartphones.
TODO: CV results?
\end{abstract}

\section{Introduction}\label{sec:intro}

Measuring color accurately is a common problem in daily life, e.g., when a surface needs to be repainted in the same color. Devices that can carry out this task, the so-called \emph{colorimeters}, are available commercially and usually cost several hundred dollars. 

On the other hand, there are smartphone apps that claim to offer the same service for free by using the built-in camera. A major challenge in this approach is how to deal with the effect of ambient light. Simply picking the color from a photo is not ideal because hue, saturation and brightness depend heavily on the lighting conditions. One way to address this issue is to capture the color of a white reference object (e.g., a white piece of paper) at the same time and use this to correct for the ambient light.

In this study, we want to find a sufficiently accurate approach to correct for ambient light and analyze the usability of measurements from such an app for color detection.
While smartphone cameras store colors in RGB format, the RGB color space is not suitable for our analysis,
because the distance between two points in the RGB space does not align with the perceived difference between two colors.
In our analysis, we use the CIELAB (or $L^* a^* b^*$) color space, designed by Commission Internationale de l’Éclairage (CIE)\footnote{
    In Enligh: International Commission on Illumination.
    Website: \url{https://cie.co.at/}.
} for colorimetric applications.
Every color in the RGB space can be mapped non-linearly to a point $(L^*, a^*, b^*)$ in the $L^* a^* b^*$ space,
where the $L^*$ coordinate represents lightness and the $a^*, b^*$ coordinates control the hue.
The Euclidean distance on the $L^* a^* b^*$ space reflects the perceived difference between colors.
According to TODO:CITE, two colors with an Euclidean distance of $2$ on the $L^* a^* b^*$ space are barely distinguishable with human eyes.
This error threshold is used to calibrate colorimeters.
As we cannot assume a smartphone app to achieve such high accuracy, we consider an Euclidean distance of $6$\todo{Or 5? Or 10?} as small color difference.

We have developed our own color measurement app with built-in correction using white reference.
As the transformation from RGB to $L^* a^* b^*$ is intended for measurements under CIE's D65 lighting condition \citep{oleari2015},
we collected data using our app under a lighting condition similar to D65 (\cref{sec:data}).
We further compared the app's correction-by-scaling method (\cref{sec:scaling}) with a modeling approach (\cref{sec:model}).
While the model provides insights into the color correction mechanism,
the scaling method proves to be more robust (\cref{sec:results}).

\begin{figure*}
    \includegraphics{../Images/graphical_abstract2.pdf}\\
    \caption{
        Workflow of data collection and color correction.
        (a) Color measurements taken by smartphone with white paper reference.
        (b) Illustration of our color detection app.
        (c) Color correction methods.
        (d) ECDFs of Euclidean distances from raw and corrected colors to the ground truths.
        The shaded areas are $95\%$ Dvoretzky–Kiefer–Wolfowitz confidence bands \cite{dvoretzky1956,wasserman2004}.
        The vertical line indicates a threshold for small perceptual color differences.
    }
    \label{fig:abstract}
\end{figure*}

\section{Data and Methods}\label{sec:methods}

% In this section, we describe the data collection procedure and define two color correction methods which we compare in subsequent analysis.

\subsection{Data Collection}\label{sec:data}

We used a color reference sheet designed for camera calibration as the object from which we collect data.
The reference sheet consists of 24 color cards and provides accurate ground-truth RGB values for each color.
We used two phones, Google Pixel 8 Pro and Samsung Galaxy S21 Ultra, for data collection.
The data were collected indoors on a desk in front of the window under natural daylight on November 19, 2025 in Tübingen, Germany.
The procedure is as follows.

We placed a white piece of paper (called \emph{white reference} in the following) with a cut-out square hole over the array of colors to ensure similar conditions for all measurements.
Only one color card was visible to the camera at any time.
We took 10 photos for each color along with the white reference and stored the pixel RGB values.
% Additionally, we also stored the mean colors of a $5 \times 5$ and $9 \times 9$ pixel squares at the same locations.\todo{
%     Do we have space left to discuss the reticle size?
% }
% After that, we moved to the next color until we collected data for all 24 colors.
In addition, the app stores the pitch and roll angle (see \cref{fig:abstract}) from which every measurement was made.
To make the procedure of taking the photos as similar as possible for the 24 colors, all measurements in one run were taken by one person,
and images were taken with a similar distribution of angles and distances over the 24 colors.
% The persons who took the images were told not to vary the yaw angle during dataset collection, since the app only detects pitch and roll.
% Taking one dataset of $10 \times 24$ images took between 10 and 20 minutes, so the daylight might vary slightly, but not strongly over that time.
The dataset we use in subsequent analysis consist of 480 measurements taken in two runs.

Our choice of lighting condition is an attempt to approximate the D65 standard lighting condition, under which the transformation from RGB to $L^* a^* b^*$ is valid.
\cref{tab:white-comparison} compares the $L^* a^* b^*$ values of the white color under the D65 condition and the white reference used in our experiment.
The measured $L^*$ value is lower than the D65 value, because the white reference in use cannot reflect all the light.
We assume the deviations in $a^*$ and $b^*$ from the standard case are sufficiently small and the transformation from RGB to $L^* a^* b^*$ is still valid.

\begin{table}
\caption{Comparison between the standard white in D65 and the white reference in our measurements (mean $\pm$ standard deviation).}
\label{tab:white-comparison}
\centering
\begin{tabular}{rccc}
    \toprule
    & $L^*$ & $a^*$ & $b^*$ \\
    \midrule
    \textbf{D65} & $100.000$ & $0.005260$ & $-0.010408$ \\
    \textbf{our} & $77.21 \pm 3.85$ & $0.30 \pm 0.93$ & $-3.53 \pm 2.07$ \\
    % \textbf{std} & $3.85248$ & $0.934964$ & $2.06641$ \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Correction by Scaling}\label{sec:scaling}

Our app is equipped with a simple, deterministic correction algorithm.
Let $(R_\textup{m}, G_{\textup{m}}, B_\textup{m})$ be the measured RGB values of a color card.
We assume that the measured color $(R_\textup{w}, G_{\textup{w}}, B_\textup{w})$ of the white reference is the brightness color one can measure under the ambient light.
To recover the full RGB space $[0, 255]^3$ from the measurable color space $[0, R_\textup{w}] \times [0, G_\textup{w}] \times [0, B_\textup{w}]$, we apply a channel-wise rescaling.
The corrected RGB values are given by
\begin{equation*}
    \begin{pmatrix}
        \widehat{R} \\ 
        \widehat{G} \\
        \widehat{B}
    \end{pmatrix} = \begin{pmatrix}
        \frac{255}{R_\textup{w}} & 0 & 0 \\
        0 & \frac{255}{G_\textup{w}} & 0 \\
        0 & 0 & \frac{255}{B_\textup{w}}
    \end{pmatrix}\begin{pmatrix}
        R_\textup{m} \\
        G_\textup{m} \\
        B_\textup{m}
    \end{pmatrix}.
\end{equation*}
This method was suggested by TODO:REF for colorimetry.

\subsection{Correction by Model}\label{sec:model}

As a more complex approach to color correction, we fit a regression model on our measured data.
To optimize the model towards reducing perceived color difference between detected and true colors, we transform the measured RGB values into the $L^* a^* b^*$ space before modeling.
We consider a multi-target linear model defined by
\begin{equation} \label{eq:model}
\begin{split}
    \begin{pmatrix}
        \widehat{L^*} \\ 
        \widehat{a^*} \\
        \widehat{b^*}
    \end{pmatrix}
    =& \begin{pmatrix}
        a_1 \\
        a_2 \\
        a_3
    \end{pmatrix} + \begin{pmatrix}
        B_{11} & B_{12} & B_{13} \\
        B_{21} & B_{22} & B_{23} \\
        B_{31} & B_{32} & B_{33}
    \end{pmatrix} \begin{pmatrix}
        L^*_\textup{m} \\
        a^*_\textup{m} \\
        b^*_\textup{m}
    \end{pmatrix} \\
    &+ \begin{pmatrix}
        C_{11} & C_{12} & C_{13} \\
        C_{21} & C_{22} & C_{23} \\
        C_{31} & C_{32} & C_{33}
    \end{pmatrix} \begin{pmatrix}
        L^*_\textup{w} \\
        a^*_\textup{w} \\
        b^*_\textup{w}
    \end{pmatrix} \\
    &+ \begin{pmatrix}
        D_{11} & D_{12} \\
        D_{21} & D_{22} \\
        D_{31} & D_{32}
    \end{pmatrix} \begin{pmatrix}
        \textup{pitch} \\
        \textup{roll}
    \end{pmatrix},
\end{split}
\end{equation}
where $(L^*_\textup{m}, a^*_\textup{m}, b^*_\textup{m})$ and $(L^*_\textup{w}, a^*_\textup{w}, b^*_\textup{w})$ are the measured colors of a color card and the white reference, respectively, and $a_{i}, B_{ij}, C_{ij}, D_{ij}$ are parameters.

The model is fitted by minimizing the empirical risk associated with the loss function
\begin{equation*} %\label{eq:euclidean-error}
    \ell(y, \hat{y})
    = \left( L^* - \widehat{L^*} \right)^2 + \left( a^* - \widehat{a^*} \right)^2 + \left( b^* - \widehat{b^*} \right)^2,
\end{equation*}
where $y = (L^*, a^*, b^*)$ denotes the ground truth color and $\hat{y} = (\widehat{L^*}, \widehat{a^*}, \widehat{b^*})$ denotes the correction given by the model.
For simplicity, we refer to $\sqrt{\ell(y, \hat{y})}$, the Euclidean distance between $y$ and $\hat{y}$, as the \emph{Euclidean error} of a correction.
For the purpose of uncertainty quantification,
we construct bootstrap confidence intervals \cite{davison1997,efron1994} for the estimated coefficients.
We resample observations with replacement\footnote{%
    This corresponds to ``resampling cases'' in \citet{davison1997} and ``bootstrap pairs'' in \citet{efron1994}.
} and construct a percentile bootstrap confidence interval per parameter.

% Pitch and roll in \eqref{eq:model} provide information about the angle from which a picture was taken.
Our assumption is that the white reference can sufficiently characterize the ambient light, so that the additional information of pitch and roll contributes little to color correction.
We validate this assumption by comparing the full model \eqref{eq:model} with a reduced model with $D = 0$.

% \begin{table}
% \caption{Targeted leave-out cross validation specification.}
% \label{tab:cv-folds}
% \centering
% \begin{tabular}{ll}
%     \toprule
%     Test set & Colors \\
%     \midrule
%     Red & 7, 9, 12, 15 \\
%     Green & 4, 6, 11, 14 \\
%     Blue & 3, 5, 6, 8, 13, 18 \\
%     Neutral & 19-24 \\
%     \bottomrule
% \end{tabular}
% \end{table}

\section{Results}\label{sec:results}

\subsection{Effect of Correction}

% \begin{figure}
%     \includegraphics{../Images/plot_error_ecdf.pdf}
%     \caption{Euclidean error ECDFs of the correction methods with $95\%$ Dvoretzky–Kiefer–Wolfowitz confidence band \cite{dvoretzky1956,wasserman2004}.}
%     \label{fig:error-ecdf}
% \end{figure}

We train the full and reduced model on $80\%$ of the measurements and test them on the remaining $20\%$.
We apply scaling correction to the same test set.
\cref{fig:abstract}(d) shows the empirical cumulative distribution function (ECDF) of the measurement-wise Euclidean error in the $L^* a^* b^*$ space on the test set.
Although the corrected colors remain visually different from the ground truths, they reduce the gap by a large margin in comparison to raw measurements.\todo{Mention some percentages?}

The error ECDFs of the two model variants are almost identical,
which indicates that adding information about pitch and roll does not change the behavior of the model, as we expected.
While the error ECDF of the models lie above that of the scaling method,
their confidence bands overlap.
At the higher end of error distributions, we see an improvement of the models over simple scaling.
The maximum error incurred by either model is lower than the scaling method.

To gain some insights into how the correction methods reduce the Euclidean error, we visualize the error per channel in \cref{fig:error-kde}.
As the two model variants yield comparable results, we only display the reduced model.
One can observe a shift of error distribution in the $L^*$ coordinates towards zero resulted from the correction methods,
whereas the error distributions in the other two channels are all centered at approximately zero.
These observations show that the correction methods are capable of removing bias towards darker colors, but cannot eliminate uncertainty in hue.

\begin{figure*}[htb]
    \includegraphics{../Images/plot_error_kde.pdf}\\
    \caption{Kernel density plots for channel-wise difference from ground truth.}
    \label{fig:error-kde}
\end{figure*}

% \begin{table}
% \caption{%
%     Statistics of Euclidean error in raw measurements and after correction.
%     Relative change to raw data (first row) are given in parentheses.
% }
% \label{tab:error-ecdf-stats}
% \centering
% \begin{tabular}{crrrr}
%     \toprule
%     Method & Mean & Median & Max \\
%     \midrule
%     (Raw) & 20.71 (0\%) & 21.46 (0\%) & 45.67 (0\%) \\
%     Scaling & 11.63 (-44\%) & 9.71 (-55\%) & 20.87 (-35\%) \\
%     Full model & 8.08 (-61\%) & 7.19 (-66\%) & 17.70 (-61\%) \\
%     Reduced model & 8.16 (-61\%) & 7.17 (-67\%) & 17.31 (-62\%) \\
%     \bottomrule
% \end{tabular}
% \end{table}

% \begin{itemize}
%     \item (Grid comparison: visual sanity check, no clear pattern cross colors. $\to$ visual abstract?)
%     \item (HSV or rather LAB) KDE plots: slightly more quantitative, but still interpretable.
%     \item Raw vs.\@ corrected RGB/LAB: scaling mostly corrects for brightness. The gray scale colors reflect the ambient light -- balanced light, as expected by the standard lighting condition.
% \end{itemize}
% KDE plots:
% \begin{itemize}
%     \item Both scaling and model mostly correct for brightness!
%     \item Hue correction is minimal, possibly because of the ambient light.
% \end{itemize}

\subsection{Generalizing to Unseen Colors}

While our dataset covers only 24 colors, the correction methods ought to generalize to arbitrary colors.
We simulate the scenario of generalization to unseen colors by employing a $k$-fold cross-validation (CV) strategy.
For each $k = 1,\dots,20$, we randomly reserve $k$ colors for testing and use the remaining $24 - k$ colors for training.
To identify colors with high impact on generalization, we perform leave-one-out CV, training the model on all but the $j$-th color for $j = 1,\dots,24$ and testing it on the single color left out.

% \begin{figure*}[tb]
%     \includegraphics{../Images/plot_cv_k_out_acc.pdf}\\
%     \caption{%
%         Leave-$k$-out cross validation (left) and leave-one-out cross validation (right) assessing model performance on unseen colors.
%         The left plot displays the performance of the reduced model on $k$ colors left out in training.
%         
%         The right plot shows the mean $\pm$ one standard deviation of validation average Euclidean error across 20 trials per left-out color.
%     }
% \end{figure*}

\begin{figure}[ht]
    \includegraphics{../Images/plot_cv_k_out_acc.pdf}\\
    \caption{Validation performance of the reduced model on $k$ left-out colors. The shaded bands show the upper and lower quartiles over 20 trials per $k$.}
    \label{fig:cv-k}
\end{figure}

\begin{figure}
    \includegraphics{../Images/plot_cv_loo.pdf}\\
    \caption{Validation average Euclidean error (mean $\pm$ one standard deviation across 20 trials) of the reduced model on single left-out color. The color of each bar is the color left out.}
\end{figure}

The results indicate the Euclidean error remains stable across a wide range of training sets sizes.
A significant inflection point in performance is observed at leaving out $k = 18$ colors from the training set,
where the MSE increases drastically.
This behavior suggests that the model maintains high predictive accuracy and robustness even when trained on a small number of colors.
We therefore believe that our results generalize beyond the 24 colors we collected data from.
% Consequently, the experimental results are highly reproducible, requiring a minimal threshold of only 5 to 6 training samples to achieve stability.

% In the targeted leave-out experiments (see \cref{tab:cv-folds}), we observe that the model trained on all colors but the green ones fail to generalize to the green colors,
% while the model readily generalizes to red or blue colors even if they are left out.
The results of leave-one-out cross validation show that color 11 (\textcolor{c11}{yellow green}) is the primary driver of failure of generalization.
Notably, color 11 is both a member of the Greenish group and possesses the largest green ($G$) channel intensity in the dataset.

% The significant surge in MSE upon the omission of greenish samples—and Sample 11 in particular—can be attributed to the non-linearities inherent in the sensor’s Green-channel response or the spectral distribution of the target illuminant. In many Bayer-filter-based sensors, the Green channel captures the majority of the luminance information and serves as the primary anchor for the transformation matrix.
% If the training set lacks these high-intensity Green samples, the model fails to capture the curvature of the sensor's response in that region, leading to poor extrapolation. Essentially, the Green samples represent a critical boundary condition; without them, the transformation becomes ill-conditioned for a large portion of the visible gamut.
% \todo[inline]{Citation!}

\section{Discussion \& Conclusion}\label{sec:conclusion}

TODO.

% \begin{itemize}
%     \item Limitations:
%     \begin{itemize}
%         \item Analysis focused on one lighting condition. We could not exactly reproduce D65 due to limited resources.
%         In particular, professional colorimeter as comparison would be meaningful.
%         \item White paper is not (255, 255, 255) -- bias towards underestimating brightness.
%         \item Factors not considered: type of smartphone, distance to object.
%         \item Error in color remains mostly uninterpretable.
%         Only comparison, but no statement about whether one method is ``good enough.''
%     \end{itemize}
% \end{itemize}

\newpage

\section*{Contribution Statement}

TODO: Explain here, in one sentence per person, what each group member contributed.

% \section*{Notes} 

% Your entire report has a \textbf{hard page limit of 4 pages} excluding references and the contribution statement. (I.e. any pages beyond page 4 must only contain the contribution statement and references). Appendices are \emph{not} possible. But you can put additional material, like interactive visualizations or videos, on a githunb repo (use \href{https://github.com/pnkraemer/tueplots}{links} in your pdf to refer to them). Each report has to contain \textbf{at least three plots or visualizations}, and \textbf{cite at least two references}. More details about how to prepare the report, inclucing how to produce plots, cite correctly, and how to ideally structure your github repo, will be discussed in the lecture, where a rubric for the evaluation will also be provided.


\bibliography{bibliography}
\bibliographystyle{icml2025}

\end{document}

% This document was modified from the files available at https://icml.cc/Conferences/2025/AuthorInstructions
% the full copyright notice is available within the file icml2025.sty